{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab7c40-c0da-4b75-a7b6-3822523f7218",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Question `2` (Decision Trees)\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| Course | Statistical Methods in AI |\n",
    "| Release Date | `19.01.2023` |\n",
    "| Due Date | `29.01.2023` |\n",
    "\n",
    "This assignment will have you working and experimenting with decision trees. Initially, you will be required to implement a decision tree classifier by choosing thresholds based on various impurity measures and reporting the scores. Later, you can experiment with the `scikit-learn` implementation of decision trees, and how various other parameters can be leveraged for better performance.\n",
    "\n",
    "The dataset is a very simple one, the [banknote authentication dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication). It has 5 columns, the first 4 are the features, and the last one is the class label. The features are the variance, skewness, curtosis and entropy of the [wavelet transformed](https://en.wikipedia.org/wiki/Wavelet_transform) image of the banknote. The class label is 1 if the banknote is authentic, and 0 if it is forged. The data is present in `bankAuth.txt`. There are a total of 1372 samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2422c78-b240-48ca-b6a0-cc8b88562670",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "01616b64-1baa-4cda-9894-5ffac51ec662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# additional imports if necessary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5653b9d8-2250-4b8e-820f-71ba9ccd5cf9",
   "metadata": {},
   "source": [
    "### Impurity Measures\n",
    "\n",
    "Decision trees are only as good as the impurity measure used to choose the best split. In this section, you will be required to implement the following impurity measures and use them to build a decision tree classifier.\n",
    "\n",
    "1. Gini Index\n",
    "2. Entropy/Log loss\n",
    "3. Misclassification Error\n",
    "\n",
    "Write functions that calculate the impurity measures for a given set of labels. The functions should take in a list of labels and return the impurity measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6a37b889-d182-441a-b7ae-0803e65aff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "def giniIndex(items):\n",
    "    zeroCount = 0\n",
    "    oneCount = 0\n",
    "    for item in items:\n",
    "        if item[-1] == 0:\n",
    "            zeroCount += 1\n",
    "        else:\n",
    "            oneCount += 1\n",
    "\n",
    "    N = float(items.shape[0])\n",
    "    if N == 0:\n",
    "        return 0\n",
    "        \n",
    "    oneFreq = oneCount/N\n",
    "    zeroFreq = zeroCount/N\n",
    "\n",
    "    gini = oneFreq*(1.0 - oneFreq) * zeroFreq*(1.0 - zeroFreq)\n",
    "    return gini\n",
    "\n",
    "def entropy(items):\n",
    "    zeroCount = 0\n",
    "    oneCount = 0\n",
    "    for item in items:\n",
    "        if item[-1] == 0:\n",
    "            zeroCount += 1\n",
    "        else:\n",
    "            oneCount += 1\n",
    "\n",
    "    N = float(items.shape[0])\n",
    "    if N == 0:\n",
    "        return 0\n",
    "\n",
    "    oneFreq = oneCount/N\n",
    "    zeroFreq = zeroCount/N\n",
    "\n",
    "    S = -(np.log(oneFreq) * oneFreq + np.log(zeroFreq)*zeroFreq) \n",
    "    return S\n",
    "\n",
    "def misclassificationError(items):\n",
    "    zeroCount = 0\n",
    "    oneCount = 0\n",
    "    for item in items:\n",
    "        if item[-1] == 0:\n",
    "            zeroCount += 1\n",
    "        else:\n",
    "            oneCount += 1\n",
    "\n",
    "    N = float(items.shape[0])\n",
    "    if N == 0:\n",
    "        return 0\n",
    "\n",
    "    oneFreq = oneCount/N\n",
    "    zeroFreq = zeroCount/N\n",
    "\n",
    "    if oneFreq > zeroFreq:\n",
    "        msce = 1 - oneFreq\n",
    "    else:\n",
    "        msce = 1 - zeroFreq\n",
    "    return msce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bfcc68-388e-4da8-ad87-270ef1212cdc",
   "metadata": {},
   "source": [
    "### Decision Tree\n",
    "\n",
    "Fit a decision tree using any one of the above impurity measures with a depth of 3. This means you will have eight leaf nodes and seven internal nodes. Report the threshold values at each internal node and the impurity measure at the final leaf node with the label. Also report the accuracy of the classifier on the training and test data (instructions for splitting the data will be given in the end)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3939308e-294f-4d0e-a1af-1589906d5129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree generation results: \n",
      "Feature:  0  | Best candidate:  -0.21210000000014517  | Impurity:  0.032640888212331694\n",
      "Feature:  1  | Best candidate:  -6.953100000000145  | Impurity:  0.0576\n",
      "Feature:  2  | Best candidate:  8.663899999999703  | Impurity:  0.057862729469445075\n",
      "Feature:  3  | Best candidate:  -8.5482  | Impurity:  0.060975190696247156\n",
      "\tPartition 0 --- Chosen feature:  0  | Candidate:  -0.21210000000014517  | Impurity:  0.032640888212331694\n",
      "Feature:  0  | Best candidate:  -0.2062  | Impurity:  0.016076188185044096\n",
      "Feature:  1  | Best candidate:  -6.9321  | Impurity:  0.016076188185044096\n",
      "Feature:  2  | Best candidate:  -4.966100000000007  | Impurity:  0.014169035610877068\n",
      "Feature:  3  | Best candidate:  -6.8103  | Impurity:  0.016076188185044096\n",
      "\tPartition 1 --- Chosen feature:  2  | Candidate:  -4.966100000000007  | Impurity:  0.014169035610877068\n",
      "Feature:  0  | Best candidate:  -7.0421  | Impurity:  0.016564700027287597\n",
      "Feature:  1  | Best candidate:  9.536899999999505  | Impurity:  0.005251489133608495\n",
      "Feature:  2  | Best candidate:  -3.7767000000000004  | Impurity:  0.015778466944861104\n",
      "Feature:  3  | Best candidate:  -7.638200000000019  | Impurity:  0.014993752603082054\n",
      "\tPartition 2 --- Chosen feature:  1  | Candidate:  9.536899999999505  | Impurity:  0.005251489133608495\n",
      "Feature:  0  | Best candidate:  -0.2062  | Impurity:  0.014169035610877068\n",
      "Feature:  1  | Best candidate:  -6.9321  | Impurity:  0.014169035610877068\n",
      "Feature:  2  | Best candidate:  -4.9417  | Impurity:  0.014169035610877068\n",
      "Feature:  3  | Best candidate:  -6.8103  | Impurity:  0.014169035610877068\n",
      "\tPartition 3 --- Chosen feature:  0  | Candidate:  -0.2062  | Impurity:  0.014169035610877068\n",
      "Feature:  0  | Best candidate:  0.25943  | Impurity:  0.0\n",
      "Feature:  1  | Best candidate:  4.5503  | Impurity:  0.0\n",
      "Feature:  2  | Best candidate:  -5.2861  | Impurity:  0.0\n",
      "Feature:  3  | Best candidate:  -6.3913  | Impurity:  0.0\n",
      "\tPartition 4 --- Chosen feature:  0  | Candidate:  0.25943  | Impurity:  0.0\n",
      "Feature:  0  | Best candidate:  -6.5135000000000005  | Impurity:  0.0\n",
      "Feature:  1  | Best candidate:  9.5663  | Impurity:  0.0003996668748843199\n",
      "Feature:  2  | Best candidate:  -3.7867  | Impurity:  0.0003996668748843199\n",
      "Feature:  3  | Best candidate:  -8.5482  | Impurity:  0.0003996668748843199\n",
      "\tPartition 5 --- Chosen feature:  0  | Candidate:  -6.5135000000000005  | Impurity:  0.0\n",
      "Feature:  0  | Best candidate:  -7.0421  | Impurity:  0.0048518222587241745\n",
      "Feature:  1  | Best candidate:  -13.7731  | Impurity:  0.0048518222587241745\n",
      "Feature:  2  | Best candidate:  -3.6778  | Impurity:  0.0048518222587241745\n",
      "Feature:  3  | Best candidate:  -7.5887  | Impurity:  0.0048518222587241745\n",
      "\tPartition 6 --- Chosen feature:  0  | Candidate:  -7.0421  | Impurity:  0.0048518222587241745\n",
      "\n",
      "Leaf nodes: \n",
      "Leaf  0  label:  0  | Impurity:  0.014169035610877068\n",
      "Leaf  1  label:  1  | Impurity:  0\n",
      "Leaf  2  label:  1  | Impurity:  0.0\n",
      "Leaf  3  label:  1  | Impurity:  0\n",
      "Leaf  4  label:  0  | Impurity:  0.0\n",
      "Leaf  5  label:  1  | Impurity:  0.0\n",
      "Leaf  6  label:  1  | Impurity:  0.0048518222587241745\n",
      "Leaf  7  label:  1  | Impurity:  0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "class Partition():\n",
    "    def __init__(self, featureNum: int, value: float):\n",
    "        self.featureNum = featureNum\n",
    "        self.value = value\n",
    "\n",
    "        if featureNum >= 4:\n",
    "            print(\"Warning: Invalid feature num\")\n",
    "\n",
    "    # pass a np array of items\n",
    "    def __call__(self, items): \n",
    "        # False -> left node, True -> right node\n",
    "        result = items[:, self.featureNum] < self.value\n",
    "\n",
    "        falseItems = items[np.where(result == False)]\n",
    "        trueItems = items[np.where(result == True)]\n",
    "\n",
    "        return [falseItems, trueItems]\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "    \"\"\"\n",
    "    Letters are groups of values, numbers are partitions\n",
    "\n",
    "                A\n",
    "                0\n",
    "           B          C\n",
    "           1          2\n",
    "        C     D    E     F\n",
    "        3     4    5     6\n",
    "      G   H  I J  K  L  M  N\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, trainData, impurity=giniIndex):\n",
    "        self.partitions = [Partition(-1,0.0)] * 7\n",
    "        self.impurity = impurity\n",
    "        self.trainData = trainData\n",
    "        self.leafNodes = []\n",
    "\n",
    "    # impurity for a specific partition\n",
    "    def getImpurity(self, items, partition : Partition):\n",
    "        falseItems, trueItems = partition(items)\n",
    "        totalImpurity = self.impurity(falseItems) + self.impurity(trueItems)\n",
    "\n",
    "        return totalImpurity\n",
    "        \n",
    "    # return the value that minimises impurity and impurity itself, for some feature\n",
    "    def findBestValue(self, items, featureNum, start=-3.0, end=3.0, stepSize=0.1):\n",
    "        lowestImpurity = np.inf\n",
    "        bestCandidate = float(start+end)/2\n",
    "\n",
    "        candidateValues = np.arange(start, end, stepSize)\n",
    "\n",
    "        for candidate in candidateValues:\n",
    "            p = Partition(featureNum, candidate)\n",
    "            impurity = self.getImpurity(items, p)\n",
    "\n",
    "            # print(candidate, impurity)\n",
    "\n",
    "            if impurity < lowestImpurity:\n",
    "                lowestImpurity = impurity\n",
    "                bestCandidate = candidate\n",
    "\n",
    "        print(\"Feature: \", featureNum, \" | Best candidate: \", bestCandidate, \" | Impurity: \", lowestImpurity)\n",
    "        return bestCandidate, lowestImpurity\n",
    "\n",
    "    # compare best choices from all features to greedily select a partition choice\n",
    "    def findBestPartition(self, items, stepSize=0.025):\n",
    "        numFeatures = items.shape[1] - 1\n",
    "        impurities = np.empty((numFeatures, 2))\n",
    "\n",
    "        for featureNum in range(numFeatures):\n",
    "            minValue = np.min(items[:, featureNum])\n",
    "            maxValue = np.max(items[:, featureNum])\n",
    "\n",
    "            bestCandidate, lowestImpurity = self.findBestValue(items, featureNum, minValue, maxValue, stepSize)\n",
    "            impurities[featureNum, :] = [bestCandidate, lowestImpurity]\n",
    "\n",
    "        chosenFeature = np.argmin(impurities[:,1])\n",
    "        chosenCandidate, bestImpurity = impurities[chosenFeature]\n",
    "\n",
    "        return chosenFeature, chosenCandidate, bestImpurity\n",
    "\n",
    "    # size is the only hard coded part of the class\n",
    "    def determinePartitions(self, stepSize=0.025):\n",
    "        # layer 1\n",
    "        chosenFeature, chosenCandidate, bestImpurity = self.findBestPartition(self.trainData, stepSize=stepSize)\n",
    "        print(\"\\tPartition 0 --- Chosen feature: \", chosenFeature, \" | Candidate: \", chosenCandidate, \" | Impurity: \", bestImpurity)\n",
    "        self.partitions[0] = Partition(chosenFeature, chosenCandidate)\n",
    "        B, C = self.partitions[0](self.trainData)\n",
    "\n",
    "        # layer 2\n",
    "        chosenFeature, chosenCandidate, bestImpurity = self.findBestPartition(B, stepSize=stepSize)\n",
    "        print(\"\\tPartition 1 --- Chosen feature: \", chosenFeature, \" | Candidate: \", chosenCandidate, \" | Impurity: \", bestImpurity)\n",
    "        self.partitions[1] = Partition(chosenFeature, chosenCandidate)\n",
    "        D, E = self.partitions[1](B)\n",
    "\n",
    "\n",
    "        chosenFeature, chosenCandidate, bestImpurity = self.findBestPartition(C, stepSize=stepSize)\n",
    "        print(\"\\tPartition 2 --- Chosen feature: \", chosenFeature, \" | Candidate: \", chosenCandidate, \" | Impurity: \", bestImpurity)\n",
    "        self.partitions[2] = Partition(chosenFeature, chosenCandidate)\n",
    "        F, G = self.partitions[2](C)\n",
    "\n",
    "        # layer 3\n",
    "        chosenFeature, chosenCandidate, bestImpurity = self.findBestPartition(D, stepSize=stepSize)\n",
    "        print(\"\\tPartition 3 --- Chosen feature: \", chosenFeature, \" | Candidate: \", chosenCandidate, \" | Impurity: \", bestImpurity)\n",
    "        self.partitions[3] = Partition(chosenFeature, chosenCandidate)\n",
    "        H, I = self.partitions[3](D)\n",
    "\n",
    "\n",
    "        chosenFeature, chosenCandidate, bestImpurity = self.findBestPartition(E, stepSize=stepSize)\n",
    "        print(\"\\tPartition 4 --- Chosen feature: \", chosenFeature, \" | Candidate: \", chosenCandidate, \" | Impurity: \", bestImpurity)\n",
    "        self.partitions[4] = Partition(chosenFeature, chosenCandidate)\n",
    "        J, K = self.partitions[4](E)\n",
    "\n",
    "\n",
    "        chosenFeature, chosenCandidate, bestImpurity = self.findBestPartition(F, stepSize=stepSize)\n",
    "        print(\"\\tPartition 5 --- Chosen feature: \", chosenFeature, \" | Candidate: \", chosenCandidate, \" | Impurity: \", bestImpurity)\n",
    "        self.partitions[5] = Partition(chosenFeature, chosenCandidate)\n",
    "        L, M = self.partitions[5](F)\n",
    "\n",
    "\n",
    "        chosenFeature, chosenCandidate, bestImpurity = self.findBestPartition(G, stepSize=stepSize)\n",
    "        print(\"\\tPartition 6 --- Chosen feature: \", chosenFeature, \" | Candidate: \", chosenCandidate, \" | Impurity: \", bestImpurity)\n",
    "        self.partitions[6] = Partition(chosenFeature, chosenCandidate)\n",
    "        N, O = self.partitions[6](G)\n",
    "\n",
    "        print(\"\\nLeaf nodes: \")\n",
    "        leaves = [H,I,J,K,L,M,N,O]\n",
    "        for i, leaf in enumerate(leaves):\n",
    "            numItems = float(leaf.shape[0])\n",
    "            numOnes = np.sum(leaf[:, -1])\n",
    "            if numOnes >= numItems/2:   # 1 is the most popular label\n",
    "                label = 1\n",
    "            else:                       # 0 is the most popular label\n",
    "                label = 0\n",
    "            print(\"Leaf \", i, \" label: \", label, \" | Impurity: \", self.impurity(leaf))\n",
    "\n",
    "        # print(\"\\tFinal impurity: \", totalFinalImpurity)\n",
    "\n",
    "    def test(self, items):\n",
    "        B, C = self.partitions[0](items)\n",
    "        D, E = self.partitions[1](B)\n",
    "        F, G = self.partitions[2](C)\n",
    "        # leaf nodes\n",
    "        H, I = self.partitions[3](D)\n",
    "        J, K = self.partitions[4](E)\n",
    "        L, M = self.partitions[5](F)\n",
    "        N, O = self.partitions[6](G)\n",
    "\n",
    "        correctlyPredicted = 0\n",
    "        totalPredicted = 0\n",
    "\n",
    "        for i in [H,I,J,K,L,M,N,O]:\n",
    "            numItems = float(i.shape[0])\n",
    "            numOnes = np.sum(i[:, -1])\n",
    "            if numOnes >= numItems/2:   # 1 is the most popular label\n",
    "                correctlyPredicted += numOnes\n",
    "            else:                       # 0 is the most popular label\n",
    "                correctlyPredicted += (numItems - numOnes)\n",
    "\n",
    "            totalPredicted += numItems\n",
    "\n",
    "        accuracy = float(correctlyPredicted)/totalPredicted\n",
    "        return accuracy, correctlyPredicted, totalPredicted\n",
    "\n",
    "        \n",
    "data = np.genfromtxt('bankAuth.txt', delimiter=',')\n",
    "print(\"Tree generation results: \")\n",
    "\n",
    "T = DecisionTree(trainData=data)\n",
    "T.determinePartitions(stepSize=0.01)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93dc80-27a8-466e-91f6-cc16d2f87187",
   "metadata": {},
   "source": [
    "### `sklearn` Decision Tree Experiments\n",
    "\n",
    "1. Scikit-learn has two decision tree implementations: [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). \n",
    "\n",
    "When would you use one over the other? What would you use in the case of the banknote authentication dataset? Explain the changes that need to be made in the dataset to use the other implementation.\n",
    "\n",
    "2. Fit a decision tree to the training set. Change various parameters and compare them to one another. Mainly try and experiment with the `criterion`, `max_depth` and `min_samples_split` parameters. Report the accuracy on the training and test set for each of the experiments while varying the parameters for comparison purposes.\n",
    "\n",
    "3. Plot your trees !! (optional) (for visualization)\n",
    "\n",
    "```python\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "def plotTree(tree):\n",
    "    \"\"\"\n",
    "    tree: Tree instance that is the result of fitting a DecisionTreeClassifier\n",
    "          or a DecisionTreeRegressor.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(30,20))\n",
    "    plot_tree(tree, filled=True, rounded=True,\n",
    "                  class_names=['forged', 'authentic'],\n",
    "                  feature_names=['var', 'skew', 'curt', 'ent'])\n",
    "    plt.show()\n",
    "    return None\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7172f816",
   "metadata": {},
   "source": [
    "## 1.\n",
    "You would use a classifier as the model is supposed to output a single class label. A decision tree regressor would produce continuous output which is meaningless for class labels.\n",
    "You could modify the dataset by labelling each entry with the confidence/probability that a note is forged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2a4f8dc0-50a7-4634-87cf-d75ccf3b43c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "data = np.genfromtxt('bankAuth.txt', delimiter=',')\n",
    "np.random.shuffle(data)\n",
    "\n",
    "trainItems, testItems = train_test_split(data, test_size=0.3)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "def testTree(clf: DecisionTreeClassifier, trainItems, testItems):\n",
    "    labels = np.array(trainItems[:,4], dtype=np.int32)\n",
    "    decision_tree = clf.fit(trainItems[:,:4], labels)\n",
    "\n",
    "    cp = 0\n",
    "    tp = testItems.shape[0]\n",
    "    \n",
    "    predictions = clf.predict(testItems[:,:4])\n",
    "    diff = testItems[:,4] - predictions         # exploiting the fact that labels are just 1 or 0\n",
    "\n",
    "    wp = np.sum(np.abs(diff))\n",
    "    cp = tp - wp\n",
    "    \n",
    "    print(\"Accuracy: \", float(cp)/tp)\n",
    "    print(cp, \" correctly predicted out of \", tp)\n",
    "\n",
    "    return float(cp)/tp, cp, tp\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d2258287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini index: \n",
      "Accuracy:  0.9320388349514563\n",
      "384.0  correctly predicted out of  412\n",
      "\n",
      "Entropy: \n",
      "Accuracy:  0.9538834951456311\n",
      "393.0  correctly predicted out of  412\n",
      "\n",
      "Log loss: \n",
      "Accuracy:  0.9538834951456311\n",
      "393.0  correctly predicted out of  412\n"
     ]
    }
   ],
   "source": [
    "## Varying criterion,\n",
    "# max_depth = 3\n",
    "\n",
    "# gini\n",
    "print(\"Gini index: \")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3), trainItems, testItems)\n",
    "\n",
    "print()\n",
    "print(\"Entropy: \")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"entropy\", max_depth=3), trainItems, testItems)\n",
    "\n",
    "print()\n",
    "print(\"Log loss: \")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"log_loss\", max_depth=3), trainItems, testItems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "c5045bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 2\n",
      "Accuracy:  0.912621359223301\n",
      "376.0  correctly predicted out of  412\n",
      "Depth = 3\n",
      "Accuracy:  0.9320388349514563\n",
      "384.0  correctly predicted out of  412\n",
      "Depth = 4\n",
      "Accuracy:  0.9611650485436893\n",
      "396.0  correctly predicted out of  412\n",
      "Depth = 5\n",
      "Accuracy:  0.9781553398058253\n",
      "403.0  correctly predicted out of  412\n",
      "Depth = 6\n",
      "Accuracy:  0.9878640776699029\n",
      "407.0  correctly predicted out of  412\n",
      "Depth = 7\n",
      "Accuracy:  0.9878640776699029\n",
      "407.0  correctly predicted out of  412\n",
      "Depth = 8\n",
      "Accuracy:  0.9878640776699029\n",
      "407.0  correctly predicted out of  412\n"
     ]
    }
   ],
   "source": [
    "## Varying max_depth, using the gini index\n",
    "\n",
    "print(\"Depth = 2\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=2), trainItems, testItems)\n",
    "\n",
    "print(\"Depth = 3\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3), trainItems, testItems)\n",
    "\n",
    "print(\"Depth = 4\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=4), trainItems, testItems)\n",
    "\n",
    "print(\"Depth = 5\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=5), trainItems, testItems)\n",
    "\n",
    "print(\"Depth = 6\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=6), trainItems, testItems)\n",
    "\n",
    "print(\"Depth = 7\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=7), trainItems, testItems)\n",
    "\n",
    "print(\"Depth = 8\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=8), trainItems, testItems)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "d4e9aab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum sample = 2\n",
      "Accuracy:  0.9320388349514563\n",
      "384.0  correctly predicted out of  412\n",
      "Minimum sample = 5\n",
      "Accuracy:  0.9320388349514563\n",
      "384.0  correctly predicted out of  412\n",
      "Minimum sample = 10\n",
      "Accuracy:  0.9320388349514563\n",
      "384.0  correctly predicted out of  412\n",
      "Minimum sample = 25\n",
      "Accuracy:  0.9320388349514563\n",
      "384.0  correctly predicted out of  412\n",
      "Minimum sample = 50\n",
      "Accuracy:  0.9271844660194175\n",
      "382.0  correctly predicted out of  412\n",
      "Minimum sample = 100\n",
      "Accuracy:  0.912621359223301\n",
      "376.0  correctly predicted out of  412\n",
      "Minimum sample = 200\n",
      "Accuracy:  0.912621359223301\n",
      "376.0  correctly predicted out of  412\n"
     ]
    }
   ],
   "source": [
    "## Varying min_split\n",
    "\n",
    "print(\"Minimum sample = 1\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=2), trainItems, testItems)\n",
    "\n",
    "print(\"Minimum sample = 2\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=2), trainItems, testItems)\n",
    "\n",
    "print(\"Minimum sample = 5\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=5), trainItems, testItems)\n",
    "\n",
    "print(\"Minimum sample = 10\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=10), trainItems, testItems)\n",
    "\n",
    "print(\"Minimum sample = 25\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=25), trainItems, testItems)\n",
    "\n",
    "print(\"Minimum sample = 50\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=50), trainItems, testItems)\n",
    "\n",
    "print(\"Minimum sample = 100\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=100), trainItems, testItems)\n",
    "\n",
    "print(\"Minimum sample = 200\")\n",
    "acc, cp, tp = testTree(DecisionTreeClassifier(criterion=\"gini\", max_depth=3, min_samples_split=200), trainItems, testItems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af48e314-8c64-4fc2-b654-726d3549e42f",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The data has been loaded onto a Pandas DataFrame. Try to get an initial feel for the data by using functions like `describe()`, `info()`, or maybe try to plot the data to check for any patterns.\n",
    "\n",
    "Note: To obtain the data from the UCI website, `wget` can be used followed by shuffling the samples using `shuf` and adding a header for easier reading via `pandas`. It is not necessary to view the data in a DataFrame and can be directly loaded onto NumPy as convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "ea8dd35b-09f6-4400-821b-d9aa0b07cd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('bankAuth.txt', delimiter=',')\n",
    "np.random.shuffle(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cec0c-7433-4d38-875d-c9b25f2366b5",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "It is a good practice to split the data into training and test sets. This is to ensure that the model is not overfitting to the training data. The test set is used to evaluate the performance of the model on unseen data. The test set is not used to train the model in any way. The test set is only used to evaluate the performance of the model. You may use the `train_test_split` function from `sklearn.model_selection` to split the data into training and test sets.\n",
    "\n",
    "It is a good idea to move your data to NumPy arrays now as it will make computing easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ffc54272-3855-4601-8ad7-f3c3062a687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "trainItems, testItems = train_test_split(data, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63938d1f-174f-4054-ba7f-82d73171f2e4",
   "metadata": {},
   "source": [
    "### Denouement\n",
    "\n",
    "Use this place to report all comparisons and wrap up the calls to the functions written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9acc1dad-a657-403b-9154-003961d241ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree generation results: \n",
      "Feature:  0  | Best candidate:  -0.21639999999893433  | Impurity:  0.03213309164899718\n",
      "Feature:  1  | Best candidate:  -6.880600000001371  | Impurity:  0.0572571034167722\n",
      "Feature:  2  | Best candidate:  8.65640000000218  | Impurity:  0.05761308759776604\n",
      "Feature:  3  | Best candidate:  -7.8719  | Impurity:  0.06120416146855296\n",
      "\tPartition 0 --- Chosen feature:  0  | Candidate:  -0.21639999999893433  | Impurity:  0.03213309164899718\n",
      "Feature:  0  | Best candidate:  -0.2062  | Impurity:  0.016094172540127037\n",
      "Feature:  1  | Best candidate:  -6.9321  | Impurity:  0.016094172540127037\n",
      "Feature:  2  | Best candidate:  -4.973599999999951  | Impurity:  0.014445191109309332\n",
      "Feature:  3  | Best candidate:  -6.8103  | Impurity:  0.016094172540127037\n",
      "\tPartition 1 --- Chosen feature:  2  | Candidate:  -4.973599999999951  | Impurity:  0.014445191109309332\n",
      "Feature:  0  | Best candidate:  -7.0364  | Impurity:  0.01603891910887014\n",
      "Feature:  1  | Best candidate:  9.60189999999535  | Impurity:  0.006543552514306556\n",
      "Feature:  2  | Best candidate:  -3.7842000000000002  | Impurity:  0.015300803044379708\n",
      "Feature:  3  | Best candidate:  -7.639399999999964  | Impurity:  0.014564528542043553\n",
      "\tPartition 2 --- Chosen feature:  1  | Candidate:  9.60189999999535  | Impurity:  0.006543552514306556\n",
      "Feature:  0  | Best candidate:  -0.2062  | Impurity:  0.014445191109309332\n",
      "Feature:  1  | Best candidate:  -6.9321  | Impurity:  0.014445191109309332\n",
      "Feature:  2  | Best candidate:  -4.9417  | Impurity:  0.014445191109309332\n",
      "Feature:  3  | Best candidate:  -6.8103  | Impurity:  0.014445191109309332\n",
      "\tPartition 3 --- Chosen feature:  0  | Candidate:  -0.2062  | Impurity:  0.014445191109309332\n",
      "Feature:  0  | Best candidate:  0.25943  | Impurity:  0.0\n",
      "Feature:  1  | Best candidate:  4.5503  | Impurity:  0.0\n",
      "Feature:  2  | Best candidate:  -5.2861  | Impurity:  0.0\n",
      "Feature:  3  | Best candidate:  -6.3913  | Impurity:  0.0\n",
      "\tPartition 4 --- Chosen feature:  0  | Candidate:  0.25943  | Impurity:  0.0\n",
      "Feature:  0  | Best candidate:  -2.6479  | Impurity:  0.0\n",
      "Feature:  1  | Best candidate:  10.1374  | Impurity:  0.0\n",
      "Feature:  2  | Best candidate:  -2.953  | Impurity:  0.0\n",
      "Feature:  3  | Best candidate:  -7.8719  | Impurity:  0.0\n",
      "\tPartition 5 --- Chosen feature:  0  | Candidate:  -2.6479  | Impurity:  0.0\n",
      "Feature:  0  | Best candidate:  -7.0364  | Impurity:  0.006543552514306556\n",
      "Feature:  1  | Best candidate:  -13.7731  | Impurity:  0.006543552514306556\n",
      "Feature:  2  | Best candidate:  -3.7842000000000002  | Impurity:  0.005919034997795327\n",
      "Feature:  3  | Best candidate:  -7.5887  | Impurity:  0.006543552514306556\n",
      "\tPartition 6 --- Chosen feature:  2  | Candidate:  -3.7842000000000002  | Impurity:  0.005919034997795327\n",
      "\n",
      "Leaf nodes: \n",
      "Leaf  0  label:  0  | Impurity:  0.014445191109309332\n",
      "Leaf  1  label:  1  | Impurity:  0\n",
      "Leaf  2  label:  1  | Impurity:  0.0\n",
      "Leaf  3  label:  1  | Impurity:  0\n",
      "Leaf  4  label:  0  | Impurity:  0.0\n",
      "Leaf  5  label:  1  | Impurity:  0\n",
      "Leaf  6  label:  1  | Impurity:  0.005919034997795327\n",
      "Leaf  7  label:  0  | Impurity:  0.0\n",
      "Leaf node accuracy:  0.9029126213592233\n",
      "372.0  correctly predicted out of  412.0\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "T = DecisionTree(trainData=trainItems)\n",
    "\n",
    "print(\"Tree generation results: \")\n",
    "T.determinePartitions(stepSize=0.0025)\n",
    "\n",
    "accuracy, cp, tp = T.test(testItems)\n",
    "print(\"Leaf node accuracy: \", accuracy)\n",
    "print(cp, \" correctly predicted out of \", tp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a4887ba97cd0453d407f2999ca251911114f05a0a638f9c2041832012853734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
