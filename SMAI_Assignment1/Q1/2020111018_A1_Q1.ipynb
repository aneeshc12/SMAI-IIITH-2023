{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NSd3vc8oPt3b"
   },
   "source": [
    "# Assignment 1\n",
    "## Question `1` (K-Nearest Neighbour)\n",
    "\n",
    "| | |\n",
    "|-|-|\n",
    "| Course | Statistical Methods in AI |\n",
    "| Release Date | `19.01.2023` |\n",
    "| Due Date | `29.01.2023` |\n",
    "\n",
    "### Instructions:\n",
    "1.   Assignment must be implemented using python notebook only (Colab , VsCode , Jupyter etc.)\n",
    "2.   You are allowed to use libraries for data preprocessing (numpy, pandas, nltk etc) and for algorithms as well (sklearn etc). You are not however allowed to directly use classifier models.\n",
    "3.   The performance of the model will hold weightage but you will also be graded largely for data preprocessing steps , explanations , feature selection for vectors etc.\n",
    "4.   Strict plagiarism checking will be done. An F will be awarded for plagiarism.\n",
    "\n",
    "### The Dataset\n",
    "The dataset is to be downloaded from the following drive link ([Link](https://drive.google.com/file/d/1u55iIrTrn41n2lv8HBjtdKLhDcy_6s7O/view?usp=sharing)).\n",
    "The dataset is a collection of *11099 tweets and has 31 columns*. The data will be in the form of a csv file which you can load in any format. The ground truth is available in the following drive link ([Link](https://drive.google.com/file/d/1--cozM5hXDwdbbDaWlB-8NqwSj0nh1Kg/view?usp=sharing)) which corresponds to whether a tweet was popular or not. Since the task involves selecting features yourself to vectorize a tweet , we suggest some data analysis of the columns you consider important.\n",
    "<br><br>\n",
    "\n",
    "### The Task\n",
    "You have to build a classifier which can predict the popularity of the tweet, i.e , if the tweet was popular or not. You are required to use **KNN** algorithm to build the classifier and cannot use any inbuilt classifier. All columns are supposed to be analyzed , filtered and preprocessed to determine its importance as a feature in the vector for every tweet (Not every column will be useful).<br>\n",
    "The Data contains the **raw text of the tweet**(in the text column) as well as other **meta data** like likes count , user followers count. Note that it might be useful to **create new columns** with useful information. For example, *number of hashtags* might be useful but is not directly present as a column.<br>\n",
    "There are 3 main sub parts:\n",
    "1. *Vectorize tweets using only meta data* - likes , user followers count , and other created data\n",
    "2. *Vectorize tweets using only it's text*. This segment will require NLP techniques to clean the text and extract a vector using a BoW model. Here is a useful link for the same - [Tf-Idf](https://towardsdatascience.com/text-vectorization-term-frequency-inverse-document-frequency-tfidf-5a3f9604da6d). Since these vectors will be very large , we recommend reducing their dimensionality (~10 - 25). Hint: [Dimentionality Reduction](https://jonathan-hui.medium.com/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491). Please note that for this also you are allowed to use libraries.\n",
    "\n",
    "3. *Combining the vectors from above two techinques to create one bigger vector*\n",
    "<br>\n",
    "\n",
    "\n",
    "Using KNN on these vectors build a classifier to predict the popularity of the tweet and report accuracies on each of the three methods as well as analysis. You can use sklearn's Nearest Neighbors and need not write KNN from scratch. (However you cannot use the classifier directly). You are expected to try the classifier for different number of neighbors and identify the optimal K value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0LJvylX8680U"
   },
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UrD1GJ6-YA5M"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eoakVIVW7EOT"
   },
   "source": [
    "## Load and display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YRwXxW4WwEL8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. ... 0. 0. 0.]\n",
      "The columns are as follows:\n",
      "created_at, id, id_str, text, truncated, entities, metadata, source, in_reply_to_status_id, in_reply_to_status_id_str, in_reply_to_user_id, in_reply_to_user_id_str, in_reply_to_screen_name, user, geo, coordinates, place, contributors, retweeted_status, is_quote_status, retweet_count, favorite_count, favorited, retweeted, lang, possibly_sensitive, quoted_status_id, quoted_status_id_str, extended_entities, quoted_status, withheld_in_countries, \n",
      "\n",
      "Sample data:\n",
      "itemID  :  1\n",
      "created_at  :  2018-07-31 13:34:40+00:00\n",
      "id  :  1024287229512953856\n",
      "id_str  :  1024287229512953856\n",
      "text  :  @hail_ee23 Thanks love its just the feeling of eyes that get me so nervous ❤️\n",
      "truncated  :  False\n",
      "entities  :  {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'hail_ee23', 'name': 'Jordan Vaughn', 'id': 927185727053553665, 'id_str': '927185727053553665', 'indices': [0, 10]}], 'urls': []}\n",
      "metadata  :  {'iso_language_code': 'en', 'result_type': 'recent'}\n",
      "source  :  <a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>\n",
      "in_reply_to_status_id  :  1.0241275270722929e+18\n",
      "in_reply_to_status_id_str  :  1.0241275270722929e+18\n",
      "in_reply_to_user_id  :  9.271857270535537e+17\n",
      "in_reply_to_user_id_str  :  9.271857270535537e+17\n",
      "in_reply_to_screen_name  :  hail_ee23\n",
      "user  :  {'id': 2407992339, 'id_str': '2407992339', 'name': 'indecent exposure', 'screen_name': 'alyssamajor9', 'location': 'Sherbrooke, Québec', 'description': 'Iifes a journey enjoy it❤️', 'url': 'https://t.co/Q7UGSdRBOO', 'entities': {'url': {'urls': [{'url': 'https://t.co/Q7UGSdRBOO', 'expanded_url': 'https://www.instagram.com/alyssa.major/', 'display_url': 'instagram.com/alyssa.major/', 'indices': [0, 23]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 199, 'friends_count': 203, 'listed_count': 1, 'created_at': 'Thu Mar 13 17:13:40 +0000 2014', 'favourites_count': 2136, 'utc_offset': None, 'time_zone': None, 'geo_enabled': True, 'verified': False, 'statuses_count': 3922, 'lang': 'en', 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1001516696467922944/uv5yDIAQ_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1001516696467922944/uv5yDIAQ_normal.jpg', 'profile_banner_url': 'https://pbs.twimg.com/profile_banners/2407992339/1527615361', 'profile_link_color': '981CEB', 'profile_sidebar_border_color': '000000', 'profile_sidebar_fill_color': '000000', 'profile_text_color': '000000', 'profile_use_background_image': False, 'has_extended_profile': True, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}\n",
      "geo  :  None\n",
      "coordinates  :  None\n",
      "place  :  {'id': '26b41b13d49ea2bf', 'url': 'https://api.twitter.com/1.1/geo/id/26b41b13d49ea2bf.json', 'place_type': 'city', 'name': 'Sherbrooke', 'full_name': 'Sherbrooke, Québec', 'country_code': 'CA', 'country': 'Canada', 'contained_within': [], 'bounding_box': {'type': 'Polygon', 'coordinates': [[[-72.10789, 45.300919], [-71.803175, 45.300919], [-71.803175, 45.524339], [-72.10789, 45.524339]]]}, 'attributes': {}}\n",
      "contributors  :  nan\n",
      "retweeted_status  :  nan\n",
      "is_quote_status  :  False\n",
      "retweet_count  :  0\n",
      "favorite_count  :  0\n",
      "favorited  :  False\n",
      "retweeted  :  False\n",
      "lang  :  en\n",
      "possibly_sensitive  :  nan\n",
      "quoted_status_id  :  nan\n",
      "quoted_status_id_str  :  nan\n",
      "extended_entities  :  nan\n",
      "quoted_status  :  nan\n",
      "withheld_in_countries  :  nan\n",
      "\n",
      "\n",
      "\n",
      "itemID  :  2\n",
      "created_at  :  2018-07-31 13:34:40+00:00\n",
      "id  :  1024287229504569344\n",
      "id_str  :  1024287229504569344\n",
      "text  :  RT @TransMediaWatch: Pink News has more on the resignation of that Labour councillor who doesn't understand the Equality Act eight years af…\n",
      "truncated  :  False\n",
      "entities  :  {'hashtags': [], 'symbols': [], 'user_mentions': [{'screen_name': 'TransMediaWatch', 'name': 'Trans Media Watch', 'id': 140509257, 'id_str': '140509257', 'indices': [3, 19]}], 'urls': []}\n",
      "metadata  :  {'iso_language_code': 'en', 'result_type': 'recent'}\n",
      "source  :  <a href=\"http://twitter.com/download/iphone\" rel=\"nofollow\">Twitter for iPhone</a>\n",
      "in_reply_to_status_id  :  nan\n",
      "in_reply_to_status_id_str  :  nan\n",
      "in_reply_to_user_id  :  nan\n",
      "in_reply_to_user_id_str  :  nan\n",
      "in_reply_to_screen_name  :  None\n",
      "user  :  {'id': 901579698223931392, 'id_str': '901579698223931392', 'name': 'Lexi L', 'screen_name': 'Archaema', 'location': 'Ohio, USA', 'description': 'I am 36, a not terribly successful author, and a fantasy/sci-fi RPG loving trans woman with a history degree - (She/Her) - Some NSFW may appear', 'url': None, 'entities': {'description': {'urls': []}}, 'protected': False, 'followers_count': 196, 'friends_count': 558, 'listed_count': 0, 'created_at': 'Sat Aug 26 22:58:45 +0000 2017', 'favourites_count': 62560, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 11546, 'lang': 'en', 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': 'F5F8FA', 'profile_background_image_url': None, 'profile_background_image_url_https': None, 'profile_background_tile': False, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1016316522812334080/oUNcDQRO_normal.jpg', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1016316522812334080/oUNcDQRO_normal.jpg', 'profile_link_color': '1DA1F2', 'profile_sidebar_border_color': 'C0DEED', 'profile_sidebar_fill_color': 'DDEEF6', 'profile_text_color': '333333', 'profile_use_background_image': True, 'has_extended_profile': True, 'default_profile': True, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}\n",
      "geo  :  None\n",
      "coordinates  :  None\n",
      "place  :  None\n",
      "contributors  :  nan\n",
      "retweeted_status  :  {'created_at': 'Tue Jul 31 11:03:32 +0000 2018', 'id': 1024249194297675776, 'id_str': '1024249194297675776', 'text': \"Pink News has more on the resignation of that Labour councillor who doesn't understand the Equality Act eight years… https://t.co/qzgZryDT5B\", 'truncated': True, 'entities': {'hashtags': [], 'symbols': [], 'user_mentions': [], 'urls': [{'url': 'https://t.co/qzgZryDT5B', 'expanded_url': 'https://twitter.com/i/web/status/1024249194297675776', 'display_url': 'twitter.com/i/web/status/1…', 'indices': [117, 140]}]}, 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'}, 'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>', 'in_reply_to_status_id': None, 'in_reply_to_status_id_str': None, 'in_reply_to_user_id': None, 'in_reply_to_user_id_str': None, 'in_reply_to_screen_name': None, 'user': {'id': 140509257, 'id_str': '140509257', 'name': 'Trans Media Watch', 'screen_name': 'TransMediaWatch', 'location': 'UK', 'description': 'Helping the media report on transgender issues with accuracy, dignity & respect. Helping trans people who are receiving media attention. We tackle the bullies!', 'url': 'http://t.co/ljQi0ck7Es', 'entities': {'url': {'urls': [{'url': 'http://t.co/ljQi0ck7Es', 'expanded_url': 'http://transmediawatch.org/', 'display_url': 'transmediawatch.org', 'indices': [0, 22]}]}, 'description': {'urls': []}}, 'protected': False, 'followers_count': 29907, 'friends_count': 1587, 'listed_count': 466, 'created_at': 'Wed May 05 17:37:41 +0000 2010', 'favourites_count': 1038, 'utc_offset': None, 'time_zone': None, 'geo_enabled': False, 'verified': False, 'statuses_count': 23218, 'lang': 'en', 'contributors_enabled': False, 'is_translator': False, 'is_translation_enabled': False, 'profile_background_color': '000000', 'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png', 'profile_background_tile': True, 'profile_image_url': 'http://pbs.twimg.com/profile_images/1707238374/TMW_blobs_with_shadow_and_text_-_400_normal.png', 'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1707238374/TMW_blobs_with_shadow_and_text_-_400_normal.png', 'profile_link_color': '22C5D6', 'profile_sidebar_border_color': '76706A', 'profile_sidebar_fill_color': 'EDEDED', 'profile_text_color': '212020', 'profile_use_background_image': True, 'has_extended_profile': False, 'default_profile': False, 'default_profile_image': False, 'following': False, 'follow_request_sent': False, 'notifications': False, 'translator_type': 'none'}, 'geo': None, 'coordinates': None, 'place': None, 'contributors': None, 'is_quote_status': False, 'retweet_count': 5, 'favorite_count': 0, 'favorited': False, 'retweeted': False, 'possibly_sensitive': False, 'lang': 'en'}\n",
      "is_quote_status  :  False\n",
      "retweet_count  :  5\n",
      "favorite_count  :  0\n",
      "favorited  :  False\n",
      "retweeted  :  False\n",
      "lang  :  en\n",
      "possibly_sensitive  :  0.0\n",
      "quoted_status_id  :  nan\n",
      "quoted_status_id_str  :  nan\n",
      "extended_entities  :  nan\n",
      "quoted_status  :  nan\n",
      "withheld_in_countries  :  nan\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_json(\"tweets.json\", lines=True)\n",
    "gt = np.loadtxt(\"ground_truth.csv\")\n",
    "print(gt)\n",
    "\n",
    "print(\"The columns are as follows:\")\n",
    "for i in data.columns:\n",
    "    print(i, end=\", \")\n",
    "\n",
    "data.insert(0, \"itemID\", range(0, len(data)))\n",
    "\n",
    "\n",
    "pd.options.display.max_seq_items = 4000\n",
    "\n",
    "print(\"\\n\\nSample data:\")\n",
    "\n",
    "a = 0\n",
    "for i,j in zip(data.keys(), data.values[1]):\n",
    "    print(i, \" : \", j)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "for x in range(len(data.values)):\n",
    "    if gt[x] == 1:\n",
    "        for i,j in zip(data.keys(), data.values[x]):\n",
    "            print(i, \" : \", j)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAH13731wPS5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywNXO3TpwQkV"
   },
   "source": [
    "## Exploratory Data Analysis\n",
    "*This is an ungraded section but is recommended to get a good grasp on the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nl8EwC77wqX4"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mztyk9Kew7q1"
   },
   "source": [
    "## Part-1\n",
    "*Vectorize tweets using only meta data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "y-rDkUtVxQph"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata only feature vector for the data split: \n",
      " [[ 9503  1606     0]\n",
      " [ 5278  1604     0]\n",
      " [10478    86     0]\n",
      " ...\n",
      " [ 5594   776 12564]\n",
      " [10115  2164     0]\n",
      " [ 6761   212     0]] (9989, 3)\n",
      "Metadata only feature vector for the test split: \n",
      " [[ 7232  1105  1025]\n",
      " [ 1209   226     0]\n",
      " [ 3464    48     0]\n",
      " ...\n",
      " [ 5796   103   346]\n",
      " [ 2072   222     0]\n",
      " [10349    14     0]] (1110, 3)\n"
     ]
    }
   ],
   "source": [
    "# relevant metadata columns\n",
    "# relCols = [\"id\", \"fcount\", \"frndcount\", \"lcount\", \"user favcount\", \"verified\", \"scount\", \"rt count\", \"fav count\", \"sens\"]\n",
    "relCols = [\"id\", \"fcount\"]#,  \"rt count\"] #,  \"frncount\", \"fav count\", ]\n",
    "\n",
    "# return data and test splits\n",
    "def getFeatures(data, percent=0.9):\n",
    "  \"\"\"\n",
    "  Funtion to return a matrix of dimensions (number of tweets, number of chosen features)\n",
    "  Input parameters to this funcion are to be chosen as per requirement (Example: Loaded dataframe of the dataset) \n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  relevant columns:\n",
    "  follower count\n",
    "  friend count\n",
    "  listed_count\n",
    "  favourite count\n",
    "  verified\n",
    "  statuses_count\n",
    "  rt count\n",
    "  fac count\n",
    "  sensitive\n",
    "  \"\"\"\n",
    "\n",
    "  vecWidth = len(relCols)\n",
    "  vecHeight = int(len(data.values))   # for data\n",
    "\n",
    "  dataFeatureVector = np.zeros((vecHeight, vecWidth), dtype=np.int32)\n",
    "\n",
    "  columns = list(data.columns)\n",
    "\n",
    "\n",
    "  testCount = 0\n",
    "  for i, item in enumerate(data.values):    \n",
    "    feature = np.zeros(vecWidth)\n",
    "\n",
    "    feature[0] = item[columns.index(\"itemID\")]                                                    # ID\n",
    "    feature[1] = item[columns.index(\"retweet_count\")]\n",
    "    # feature[1] = item[columns.index(\"user\")][\"followers_count\"]\n",
    "    # feature[3] = item[columns.index(\"user\")][\"friends_count\"]\n",
    "    # feature[4] = item[columns.index(\"favorite_count\")]\n",
    "    # feature[5] = item[columns.index(\"user\")][\"friends_count\"]\n",
    "    # feature[6] = item[columns.index(\"user\")][\"verified\"]\n",
    "    # feature[7] = item[columns.index(\"user\")][\"statuses_count\"]\n",
    "    # feature[8] = item[columns.index(\"retweet_count\")]\n",
    "    # feature[9] = item[columns.index(\"favorite_count\")]\n",
    "    \n",
    "    dataFeatureVector[i] = feature\n",
    "\n",
    "  return dataFeatureVector\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "fullVector = getFeatures(data)\n",
    "featureVector = fullVector[:int(0.9*fullVector.shape[0])]\n",
    "testFeatureVector = fullVector[int(0.9*fullVector.shape[0]):]\n",
    "\n",
    "print(\"Metadata only feature vector for the data split: \\n\", featureVector, featureVector.shape)\n",
    "print(\"Metadata only feature vector for the test split: \\n\", testFeatureVector, testFeatureVector.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4K7Fzh5Q4wrV"
   },
   "source": [
    "Perform KNN using the vector obtained from get_features() function. Following are the steps to be followed:\n",
    "1. Normalise the vectors\n",
    "2. Split the data into training and test to estimate the performance.\n",
    "3. Fit the Nearest Neughbiurs module to the training data and obtain the predicted class by getting the nearest neighbours on the test data.\n",
    "4. Report the accuracy, chosen k-value and method used to obtain the predicted class. Hint: Plot accuracies for a range of k-values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "EBHGbdsi4fe-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalised data feature vector:\n",
      " [[9.5030000e+03 3.6767946e-05 0.0000000e+00]\n",
      " [5.2780000e+03 3.6722158e-05 0.0000000e+00]\n",
      " [1.0478000e+04 1.9688937e-06 0.0000000e+00]\n",
      " ...\n",
      " [5.5940000e+03 1.7765833e-05 4.5195647e-04]\n",
      " [1.0115000e+04 4.9542861e-05 0.0000000e+00]\n",
      " [6.7610000e+03 4.8535521e-06 0.0000000e+00]] \n",
      "\n",
      "Normalised test feature vector:\n",
      " [[7.2320000e+03 4.8009239e-05 3.3790455e-04]\n",
      " [1.2090000e+03 9.8190840e-06 0.0000000e+00]\n",
      " [3.4640000e+03 2.0854693e-06 0.0000000e+00]\n",
      " ...\n",
      " [5.7960000e+03 4.4750695e-06 1.1406339e-04]\n",
      " [2.0720000e+03 9.6452950e-06 0.0000000e+00]\n",
      " [1.0349000e+04 6.0826187e-07 0.0000000e+00]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 1 (normalising)\n",
    "\n",
    "def normaliseFeatureVector(featureVector):\n",
    "    normFeatureVector = np.zeros_like(featureVector, dtype=np.float32)\n",
    "\n",
    "    _, nCols = featureVector.shape\n",
    "\n",
    "    normFeatureVector[:, 0] = featureVector[:, 0] # ID\n",
    "    for i in range(1, nCols):\n",
    "        sum = np.sum(featureVector[:, i])\n",
    "\n",
    "        if sum != 0:\n",
    "            normFeatureVector[:, i] = featureVector[:, i] / sum\n",
    "\n",
    "        else:\n",
    "            print(\"Zero sum for column %i\" % i)\n",
    "            normFeatureVector[:, i] = featureVector[:, i]\n",
    "\n",
    "    return normFeatureVector\n",
    "\n",
    "# Part 2 (split)\n",
    "# using a 90/10 training(data)/test(eval) split\n",
    "\n",
    "fullVector = getFeatures(data, percent=0.7)\n",
    "dataFeatureVector = fullVector[:int(0.9*fullVector.shape[0])]\n",
    "testFeatureVector = fullVector[int(0.9*fullVector.shape[0]):]\n",
    "\n",
    "nDataFeatureVector = normaliseFeatureVector(dataFeatureVector)\n",
    "nTestFeatureVector = normaliseFeatureVector(testFeatureVector)\n",
    "\n",
    "print(\"Normalised data feature vector:\\n\", nDataFeatureVector, \"\\n\")\n",
    "print(\"Normalised test feature vector:\\n\", nTestFeatureVector, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1264.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  1  | Accuracy =  0.8801801801801802  | Correctly predicted positives =  0.05128205128205128  | kp =  4  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1249.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  2  | Accuracy =  0.8216216216216217  | Correctly predicted positives =  0.23076923076923078  | kp =  18  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1287.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  3  | Accuracy =  0.9117117117117117  | Correctly predicted positives =  0.01282051282051282  | kp =  1  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1308.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  4  | Accuracy =  0.8918918918918919  | Correctly predicted positives =  0.02564102564102564  | kp =  2  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1305.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  5  | Accuracy =  0.9252252252252252  | Correctly predicted positives =  0.0  | kp =  0  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1309.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  6  | Accuracy =  0.9171171171171171  | Correctly predicted positives =  0.0  | kp =  0  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1314.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  7  | Accuracy =  0.9288288288288288  | Correctly predicted positives =  0.0  | kp =  0  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1318.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  8  | Accuracy =  0.927027027027027  | Correctly predicted positives =  0.0  | kp =  0  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:00<00:00, 1272.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K =  9  | Accuracy =  0.9297297297297298  | Correctly predicted positives =  0.0  | kp =  0  out of  78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Part 3 (fitting and determining k)\n",
    "\n",
    "def calcL2Norm(a, b):\n",
    "    if len(a) != len(b):\n",
    "        print(\"Incorrect dim\")\n",
    "        exit(1)\n",
    "    \n",
    "    sum = 0\n",
    "    for x in range(1,len(a)):\n",
    "        sum += (a[x]-b[x]) * (a[x]-b[x])\n",
    "    \n",
    "    return math.sqrt(sum)\n",
    "\n",
    "\n",
    "def kNN(k, nDataFeatureVector, nTestFeatureVector):\n",
    "    correctPredictions = 0\n",
    "    correctPositive = 0\n",
    "    totalPredictions = len(nTestFeatureVector)\n",
    "    totalPositives = 0\n",
    "\n",
    "    # get the features with the k smallest costs\n",
    "    for testEntry in tqdm(nTestFeatureVector):\n",
    "        allCosts = nDataFeatureVector - testEntry\n",
    "        allCosts = np.square(allCosts)\n",
    "        allCosts = np.sum(allCosts[:, 1:], axis=1)        \n",
    "\n",
    "        sortedCosts = np.zeros((allCosts.shape[0], 2))\n",
    "        sortedCosts[:,0] = np.sqrt(allCosts)\n",
    "        sortedCosts[:,1] = np.arange(len(nDataFeatureVector))                \n",
    "\n",
    "        ind = np.argsort(sortedCosts[:,0])\n",
    "        sortedCosts = sortedCosts[ind]\n",
    "\n",
    "        closest = sortedCosts[:k]\n",
    "        # check nbd classes\n",
    "        tot = 0.0\n",
    "        predictedClass = 0\n",
    "\n",
    "        bleep = 0\n",
    "        for i in closest:\n",
    "            # print(i.shape)\n",
    "            # print(i)\n",
    "            # print(gt[int(i[1])])\n",
    "            if gt[int(i[1])] == 1.0:\n",
    "                bleep += 1\n",
    "            tot += gt[int(i[1])]\n",
    "        \n",
    "        # if bleep > 0:\n",
    "            # print(bleep, testEntry[0])\n",
    "\n",
    "        if float(tot)/k < 0.5:\n",
    "            predictedClass = 0\n",
    "        else:\n",
    "            predictedClass = 1\n",
    "\n",
    "        if gt[int(testEntry[0])] == predictedClass:\n",
    "            correctPredictions += 1\n",
    "            if predictedClass == 1:\n",
    "                correctPositive += 1\n",
    "\n",
    "        if gt[int(testEntry[0])] == 1:\n",
    "            totalPositives += 1\n",
    "    \n",
    "    accuracy = float(correctPredictions)/totalPredictions\n",
    "    positiveAccuracy = float(correctPositive)/totalPositives\n",
    "    return accuracy, positiveAccuracy, correctPositive, totalPositives\n",
    "\n",
    "# loop over all ks, calculate accuracy and percent of correctly predicted positives\n",
    "maxK = 10\n",
    "for k in range(1, maxK):\n",
    "    accuracy, positiveAccuracy, correctPositive, tp = kNN(k, nDataFeatureVector, nTestFeatureVector)\n",
    "    print(\"K = \", k, \" | Accuracy = \", accuracy, \" | Correctly predicted positives = \", positiveAccuracy, \" | kp = \", correctPositive, \" out of \", tp)\n",
    "        \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the chosen metadata fields, the optimal value of k seems to be 2.\n",
    "The actual accuracy is not a good metric due to how heavily skewed the data is to not popular tweets. Instead, the number of correctly predicted positives is included as a metric.\n",
    "Past a certain value of k, the number of non popular tweets in the nearest neighbours greatly outweighs the closeby positive tweets, thus no tweet is ever predicted to be popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSufXze8zmV3"
   },
   "source": [
    "## Part-2\n",
    "Vectorize tweets based on the text. More details and reference links can be checked on the Tasks list in the start of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GBizLGhg0kQT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11099, 1710)\n",
      "Important words:  ['people' 'new' 'like' ... 'winners' 'writer' 'zimbabwe']\n",
      "Performing SVD...\n",
      "SVD complete\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def tweet_vectoriser(data, percent=0.9):\n",
    "  \"\"\"\n",
    "  Funtion to return a matrix of dimensions (number of tweets, number of features extracted per tweet)\n",
    "  Following are the steps for be followed:\n",
    "    1. Remove links, tags and hashtags from each tweet.\n",
    "    2. Apply TF-IDF on the tweets to extract a vector. \n",
    "    3. Perform dimensionality reduction on the obtained vector. \n",
    "  Input parameters to this funcion are to be chosen as per requirement (Example: Array of tweets) \n",
    "  \"\"\"\n",
    "  \n",
    "  # clean text\n",
    "  cleanText = []\n",
    "  columns = list(data.columns)\n",
    "\n",
    "  for i in data.values:\n",
    "    text = i[columns.index(\"text\")]\n",
    "\n",
    "    removalRegex = \"@[a-z0-9]*|[?!\\.\\*]*| [a-z0-9]*…$|^rt|[ ]{2,}|https:\\/\\/.*|\\&amp|\\\\n\"\n",
    "    leaveNothingBehind = \"[^a-zA-Z\\- #\\n]\"\n",
    "    removeUncommonStops = \" im | got | just | ive | th | hes | shes | its | dont | do |   *\"\n",
    "\n",
    "    text = text.lower()\n",
    "    text = re.sub(removalRegex , '', text)\n",
    "    text = re.sub(leaveNothingBehind , '', text)\n",
    "    text = re.sub(removeUncommonStops , '', text)\n",
    "\n",
    "    cleanText.append([i[0], text])\n",
    "  \n",
    "  # tf-idf\n",
    "  cleanTextDF = pd.DataFrame(cleanText, columns=[\"itemID\", \"text\"])\n",
    "  vectorizer = CountVectorizer(stop_words='english') \n",
    "  counts = vectorizer.fit_transform(cleanTextDF.loc[:, \"text\"]) \n",
    "  countsDf = pd.DataFrame(counts.A, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "  # drop uncommon words, get most important ones sorted\n",
    "  allWords = list(countsDf.columns)\n",
    "  recurringImportantWords = []\n",
    "  \n",
    "  # remove infrequent words\n",
    "  for i, item in enumerate(countsDf.sum()):\n",
    "    if(item >= 10):\n",
    "      recurringImportantWords.append([allWords[i], item])\n",
    "\n",
    "  recurringImportantWords = sorted(recurringImportantWords, key=lambda x: -x[1])\n",
    "            # print(len(recurringImportantWords), recurringImportantWords)\n",
    "            # print(np.array(recurringImportantWords[1]))\n",
    "\n",
    "  # create TF-IDF vectors for each tweet\n",
    "  tfidfVector = np.zeros((len(data.values), len(recurringImportantWords)), dtype=np.float32)\n",
    "  indexList = []\n",
    "  print(tfidfVector.shape)\n",
    "\n",
    "  N = len(cleanTextDF.values)\n",
    "  idf = np.log(N * np.reciprocal(np.array(np.array(recurringImportantWords)[:,1], dtype=np.float32)))\n",
    "\n",
    "  onlyWords = np.array(recurringImportantWords)[:,0]\n",
    "  print(\"Important words: \", onlyWords)\n",
    "\n",
    "\n",
    "  for i, value in enumerate(cleanTextDF.values):\n",
    "    # print(i,end=\"\\r\")\n",
    "    words = value[1]\n",
    "    indexList.append(value[0])\n",
    "\n",
    "    # get per sentence word counts\n",
    "    for word in words.split(\" \"):\n",
    "      present = np.where(onlyWords == word)\n",
    "\n",
    "      if len(present[0]) == 0:\n",
    "        continue\n",
    "      else:\n",
    "        tfidfVector[i][int(present[0])] += 1\n",
    "\n",
    "\n",
    "    print(\"Sentence: \", i, end=\"\\r\")\n",
    "    tfidfVector[i] = np.log(1 + tfidfVector[i])\n",
    "    tfidfVector[i] = tfidfVector[i] * idf\n",
    "\n",
    "  # dimensionality reduction\n",
    "\n",
    "  # normalise tfids first\n",
    "  # normalise\n",
    "  for i in range(1, tfidfVector.shape[1]):\n",
    "    avg = np.mean(tfidfVector[:,i])\n",
    "    tfidfVector[:,i] -= avg\n",
    "\n",
    "    bigness = np.max(tfidfVector[:,i]) - np.min(tfidfVector[:,i])\n",
    "\n",
    "    if bigness != 0:\n",
    "      tfidfVector[:,i] /= bigness\n",
    "\n",
    "  print(\"Performing SVD...\")\n",
    "  u, s, v = np.linalg.svd(tfidfVector)\n",
    "  numDimensions = 25\n",
    "\n",
    "  print(\"SVD complete\")\n",
    "\n",
    "  reducedTfidVector = np.empty((u.shape[0], numDimensions + 1))\n",
    "  reducedTfidVector[:,0] = indexList\n",
    "  reducedTfidVector[:,1:] = u[:,:numDimensions]\n",
    "\n",
    "  return reducedTfidVector\n",
    "   \n",
    "\n",
    "\n",
    "# relevant metadata columns\n",
    "# relCols = [\"id\", \"fcount\", \"frndcount\", \"lcount\", \"user favcount\", \"verified\", \"scount\", \"rt count\", \"fav count\", \"sens\"]\n",
    "relCols = [\"id\", \"fcount\",  \"frncount\", \"rt count\", \"fav count\", ]\n",
    "\n",
    "# return data and test splits\n",
    "def getFeatures(data, percent=0.9):\n",
    "  \"\"\"\n",
    "  Funtion to return a matrix of dimensions (number of tweets, number of chosen features)\n",
    "  Input parameters to this funcion are to be chosen as per requirement (Example: Loaded dataframe of the dataset) \n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "  relevant columns:\n",
    "  follower count\n",
    "  friend count\n",
    "  listed_count\n",
    "  favourite count\n",
    "  verified\n",
    "  statuses_count\n",
    "  rt count\n",
    "  fac count\n",
    "  sensitive\n",
    "  \"\"\"\n",
    "\n",
    "  vecWidth = len(relCols)\n",
    "  vecHeight = int(len(data.values) * percent)   # for data\n",
    "\n",
    "  dataFeatureVector = np.zeros((vecHeight, vecWidth), dtype=np.int32)\n",
    "  testFeatureVector = np.zeros((len(data.values) - vecHeight, vecWidth), dtype=np.int32)\n",
    "\n",
    "  columns = list(data.columns)\n",
    "\n",
    "\n",
    "  testCount = 0\n",
    "  for i, item in enumerate(data.values):    \n",
    "    feature = np.zeros(vecWidth)\n",
    "\n",
    "    feature[0] = item[columns.index(\"itemID\")]                                                    # ID\n",
    "    feature[1] = item[columns.index(\"user\")][\"followers_count\"]\n",
    "    feature[2] = item[columns.index(\"user\")][\"friends_count\"]\n",
    "    feature[3] = item[columns.index(\"retweet_count\")]\n",
    "    feature[4] = item[columns.index(\"favorite_count\")]\n",
    "    # feature[5] = item[columns.index(\"user\")][\"friends_count\"]\n",
    "    # feature[6] = item[columns.index(\"user\")][\"verified\"]\n",
    "    # feature[7] = item[columns.index(\"user\")][\"statuses_count\"]\n",
    "    # feature[8] = item[columns.index(\"retweet_count\")]\n",
    "    # feature[9] = item[columns.index(\"favorite_count\")]\n",
    "    \n",
    "    if i < vecHeight:\n",
    "      dataFeatureVector[i] = feature\n",
    "    else:\n",
    "      testFeatureVector[testCount] = feature\n",
    "      testCount += 1\n",
    "\n",
    "  return dataFeatureVector, testFeatureVector\n",
    "\n",
    "data = data.sample(frac=1)\n",
    "\n",
    "reducedTfidfVector= tweet_vectoriser(data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Q47SVkv9VzO"
   },
   "source": [
    "Perform KNN using the vector obtained from tweet_vectoriser() function. Following are the steps to be followed:\n",
    "\n",
    "1. Normalise the vectors\n",
    "2. Split the data into training and test to estimate the performance.\n",
    "3. Fit the Nearest Neughbiurs module to the training data and obtain the predicted class by getting the nearest neighbours on the test data.\n",
    "4. Report the accuracy, chosen k-value and method used to obtain the predicted class. Hint: Plot accuracies for a range of k-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced tfid vecotr:  [[ 3.77900000e+03  1.84468802e-02  5.89117135e-03 ... -1.06675952e-02\n",
      "  -1.39467266e-02 -8.27888883e-02]\n",
      " [ 1.03680000e+04  1.81541230e-02  7.16250849e-03 ...  1.37821968e-01\n",
      "  -1.38169090e-02  1.35356766e-01]\n",
      " [ 4.17600000e+03  1.83697598e-02 -9.36803597e-02 ... -4.88249747e-03\n",
      "  -2.73822095e-02  4.12887626e-02]\n",
      " ...\n",
      " [ 3.65800000e+03  1.83151175e-02  4.75044741e-03 ... -1.31452876e-04\n",
      "  -1.64250366e-03 -2.04305443e-02]\n",
      " [ 1.10000000e+02  1.90025041e-02  7.04367740e-03 ...  8.11255765e-04\n",
      "   3.83277688e-03 -2.89465572e-02]\n",
      " [ 8.08900000e+03  1.83351230e-02  6.09017296e-03 ... -2.49387832e-03\n",
      "  -1.01815991e-03 -1.81536116e-02]]\n"
     ]
    }
   ],
   "source": [
    "# normalise\n",
    "for i in range(1, reducedTfidfVector.shape[1]):\n",
    "  avg = np.mean(reducedTfidfVector[:,i])\n",
    "  reducedTfidfVector[:,i] -= avg\n",
    "\n",
    "  bigness = np.max(reducedTfidfVector[:,i]) - np.min(reducedTfidfVector[:,i])\n",
    "  reducedTfidfVector[:,i] /= bigness\n",
    "\n",
    "print(\"Reduced tfid vecotr: \", reducedTfidfVector)\n",
    "\n",
    "pd.options.display.max_colwidth = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "7aDr7vAW-NX_"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 788.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  6  | pptn:  72  | pntp:  85  | pntn:  947\n",
      "K =  1  | Accuracy =  0.8585585585585586  | Correctly predicted positives % =  0.06593406593406594  | correct positive number =  6  out of  91  | pp =  78  | pn =  1032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 811.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  15  | pptn:  129  | pntp:  76  | pntn:  890\n",
      "K =  2  | Accuracy =  0.8153153153153153  | Correctly predicted positives % =  0.16483516483516483  | correct positive number =  15  out of  91  | pp =  144  | pn =  966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 854.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  1  | pptn:  18  | pntp:  90  | pntn:  1001\n",
      "K =  3  | Accuracy =  0.9027027027027027  | Correctly predicted positives % =  0.01098901098901099  | correct positive number =  1  out of  91  | pp =  19  | pn =  1091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 845.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  1  | pptn:  37  | pntp:  90  | pntn:  982\n",
      "K =  4  | Accuracy =  0.8855855855855855  | Correctly predicted positives % =  0.01098901098901099  | correct positive number =  1  out of  91  | pp =  38  | pn =  1072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 856.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  0  | pptn:  6  | pntp:  91  | pntn:  1013\n",
      "K =  5  | Accuracy =  0.9126126126126126  | Correctly predicted positives % =  0.0  | correct positive number =  0  out of  91  | pp =  6  | pn =  1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 836.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  0  | pptn:  10  | pntp:  91  | pntn:  1009\n",
      "K =  6  | Accuracy =  0.909009009009009  | Correctly predicted positives % =  0.0  | correct positive number =  0  out of  91  | pp =  10  | pn =  1100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 858.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  0  | pptn:  0  | pntp:  91  | pntn:  1019\n",
      "K =  7  | Accuracy =  0.918018018018018  | Correctly predicted positives % =  0.0  | correct positive number =  0  out of  91  | pp =  0  | pn =  1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 791.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  0  | pptn:  3  | pntp:  91  | pntn:  1016\n",
      "K =  8  | Accuracy =  0.9153153153153153  | Correctly predicted positives % =  0.0  | correct positive number =  0  out of  91  | pp =  3  | pn =  1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1110/1110 [00:01<00:00, 844.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pptp:  0  | pptn:  0  | pntp:  91  | pntn:  1019\n",
      "K =  9  | Accuracy =  0.918018018018018  | Correctly predicted positives % =  0.0  | correct positive number =  0  out of  91  | pp =  0  | pn =  1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# performing a k-nn search\n",
    "# data was shuffled initially, no need to shuffle again\n",
    "\n",
    "dataTfidf = reducedTfidfVector[ :int(0.90 * reducedTfidfVector.shape[0])  , :]\n",
    "testTfidf = reducedTfidfVector[  int(0.90 * reducedTfidfVector.shape[0]): , :]\n",
    "\n",
    "def textKNN(k, dataTfidf, testTfidf):\n",
    "    correctPredictions = 0\n",
    "    correctPositive = 0\n",
    "    totalPredictions = len(nTestFeatureVector)\n",
    "    totalPositives = 0\n",
    "    predictedPositives = 0\n",
    "    predictedNegatives = 0\n",
    "\n",
    "    pptp = 0\n",
    "    pntp = 0\n",
    "    pptn = 0\n",
    "    pntn = 0\n",
    "\n",
    "    # get the features with the k smallest costs\n",
    "    for testEntry in tqdm(testTfidf):\n",
    "        allCosts = dataTfidf - testEntry\n",
    "        allCosts = np.square(allCosts)\n",
    "        allCosts = np.sum(allCosts[:, 1:], axis=1)          # 1st column is the ID\n",
    "\n",
    "        sortedCosts = np.zeros((allCosts.shape[0], 2))\n",
    "        sortedCosts[:,0] = np.sqrt(allCosts)\n",
    "        sortedCosts[:,1] = dataTfidf[:,0]\n",
    "\n",
    "        ind = np.argsort(sortedCosts[:,0])\n",
    "        sortedCosts = sortedCosts[ind]\n",
    "\n",
    "        closest = sortedCosts[:k,:]\n",
    "        # check nbd classes\n",
    "        tot = 0.0\n",
    "        predictedClass = 0\n",
    "\n",
    "        bleep = 0\n",
    "        for i in closest:\n",
    "            # print(i.shape)\n",
    "            # print(i)\n",
    "            # print(gt[int(i[1])])\n",
    "            if gt[int(i[1])] == 1.0:\n",
    "                bleep += 1\n",
    "            tot += gt[int(i[1])]\n",
    "        \n",
    "        # if bleep > 0:\n",
    "            # print(bleep, testEntry[0])\n",
    "\n",
    "        if float(tot)/k < 0.5:\n",
    "            predictedClass = 0\n",
    "        else:\n",
    "            predictedClass = 1\n",
    "\n",
    "        if gt[int(testEntry[0])] == predictedClass:\n",
    "            correctPredictions += 1\n",
    "            if predictedClass == 1:\n",
    "                correctPositive += 1\n",
    "\n",
    "        if gt[int(testEntry[0])] == 1:\n",
    "            totalPositives += 1\n",
    "\n",
    "        if predictedClass == 1:\n",
    "            predictedPositives += 1\n",
    "            if gt[int(testEntry[0])] == 0:\n",
    "                pptn += 1\n",
    "            else:\n",
    "                pptp += 1\n",
    "\n",
    "        else:\n",
    "            predictedNegatives += 1\n",
    "            if gt[int(testEntry[0])] == 0:\n",
    "                pntn += 1\n",
    "            else:\n",
    "                pntp += 1\n",
    "    \n",
    "    \n",
    "\n",
    "    print(\"pptp: \", pptp, \" | pptn: \", pptn, \" | pntp: \", pntp, \" | pntn: \", pntn)\n",
    "    accuracy = float(pptp + pntn)/(pptp + pptn + pntp + pntn)\n",
    "    positiveAccuracy = float(pptp)/(pptp+pntp)\n",
    "    return accuracy, positiveAccuracy, correctPositive, totalPositives, predictedPositives, predictedNegatives\n",
    "\n",
    "# loop over all ks, calculate accuracy and percent of correctly predicted positives\n",
    "maxK = 10\n",
    "for k in range(1, 10):\n",
    "    accuracy, positiveAccuracy, correctPositive, tp, pp, pn = textKNN(k, dataTfidf, testTfidf)\n",
    "    print(\"K = \", k, \" | Accuracy = \", accuracy, \" | Correctly predicted positives % = \", positiveAccuracy, \" | correct positive number = \", correctPositive, \" out of \", tp, \" | pp = \", pp, \" | pn = \", pn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rg7hD8-O3PbO"
   },
   "source": [
    "## Part-3\n",
    "### Subpart-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bp9TVeVD9lKe"
   },
   "source": [
    "Combine both the vectors obtained from the tweet_vectoriser() and get_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "r5ksyj7_3_xl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "def combineVectors(dataVector, tfidfVector):\n",
    "    combinedVector = np.empty((dataVector.shape[0], dataVector.shape[0] + tfidfVector.shape[0] - 1))\n",
    "    for dataEntry, tfidfEntry in zip(dataVector, tfidfVector):\n",
    "        combinedVector[dataEntry[0],:dataEntry.shape] = dataEntry\n",
    "        combinedVector[tfidfEntry[0], dataEntry.shape:] = tfidfEntry[:-1]\n",
    "    \n",
    "    return combinedVector\n",
    "\n",
    "combineVectors(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swCD5Jp69xo5"
   },
   "source": [
    "Perform KNN using the vector obtained in the previous step. Following are the steps to be followed:\n",
    "\n",
    "1. Normalise the vectors\n",
    "2. Split the data into training and test to estimate the performance.\n",
    "3. Fit the Nearest Neughbiurs module to the training data and obtain the predicted class by getting the nearest neighbours on the test data.\n",
    "4. Report the accuracy, chosen k-value and method used to obtain the predicted class. Hint: Plot accuracies for a range of k-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rufknWgo4AvY"
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1NVQLLhE3c79"
   },
   "source": [
    "### Subpart-2\n",
    "\n",
    "Explain the differences between the accuracies obtained in each part above based on the features used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VkiTeqaE_4ic"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "2a4887ba97cd0453d407f2999ca251911114f05a0a638f9c2041832012853734"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
